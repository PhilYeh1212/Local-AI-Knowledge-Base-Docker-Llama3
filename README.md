# Local-AI-Knowledge-Base-Docker-Llama3
A production-ready, 100% offline RAG Knowledge Base using Docker, Llama 3, and Ollama. Chat with your documents privately. Enterprise architecture showcase.



# ğŸ§  Enterprise Local RAG: Private AI Knowledge Base (Docker + Llama 3)

[![Docker](https://img.shields.io/badge/Docker-Containerized-blue.svg)](https://www.docker.com/)
[![Llama 3](https://img.shields.io/badge/Model-Llama%203-blueviolet.svg)](https://ai.meta.com/llama/)
[![License](https://img.shields.io/badge/License-Commercial-green.svg)]()
[![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux-lightgrey.svg)]()

> **Stop sending your sensitive engineering data to the cloud.**
>
> This project provides a production-grade, **100% offline RAG (Retrieval-Augmented Generation)** architecture. It allows you to chat with your proprietary documents (PDF, TXT, Markdown) using a local LLM, ensuring absolute data privacy.

---

## ğŸ—ï¸ System Architecture

This system is designed with a microservices architecture, fully containerized using **Docker Compose** for one-click deployment.

![System Architecture Diagram](images/architecture_diagram.png)
*(Place a diagram here showing: User -> Web UI -> Backend API -> ChromaDB <-> Ollama)*

### ğŸ› ï¸ Tech Stack
* **LLM Inference:** [Ollama](https://ollama.com/) (Running **Meta Llama 3** 8B)
* **Embeddings:** `mxbai-embed-large` (State-of-the-art retrieval performance)
* **Vector Database:** [ChromaDB](https://www.trychroma.com/) (Persistent local storage)
* **Backend/Frontend:** Python + Streamlit (Optimized for RAG workflows)
* **Deployment:** Docker Compose (Isolated environment)

---

## âœ¨ Key Features

* **ğŸ”’ 100% Privacy:** No data leaves your machine. No OpenAI API keys required. Zero monthly fees.
* **ğŸš€ GPU Acceleration:** Native support for NVIDIA GPUs (CUDA) for lightning-fast inference.
* **ğŸ“‚ Smart Ingestion:** Automatically parses, chunks, and vectorizes PDF and text documents.
* **ğŸ’¬ Context-Aware Chat:** Remembers conversation history and retrieves relevant context from your knowledge base.
* **ğŸ³ One-Click Setup:** No "dependency hell". Just run `docker-compose up -d`.

---

## ğŸ“¸ Screenshots

### 1. Chat Interface (Streamlit)
![Chat UI](images/chat_demo.png)

### 2. Knowledge Base Management
![Ingestion Process](images/ingestion_demo.png)

---

## ğŸ’» System Requirements

To run this system smoothly with Llama 3 (8B), the following hardware is recommended:

* **OS:** Windows 10/11 (WSL2) or Linux (Ubuntu)
* **RAM:** 16GB+ System Memory
* **GPU:** NVIDIA RTX 3060 (8GB VRAM) or higher recommended.
    * *Note: The system can run on CPU-only mode, but inference will be slower.*

---

## ğŸ“¥ Get the Complete System

Building a stable RAG system from scratch takes weeks of configuration (handling Python dependencies, Vector DB connections, and Docker networking).

I have packaged the **Full Source Code**, **Docker Configuration**, and **Setup Guide** into a ready-to-deploy bundle.

### ğŸ“¦ What's included in the Full Package?
* âœ… **Complete Source Code** (Python)
* âœ… **`docker-compose.yml`** (Production ready)
* âœ… **Embedding & Vectorization Logic**
* âœ… **UI/UX Implementation**
* âœ… **Premium Support Guide**

ğŸ‘‰ **Download the System Here:** [**Get it on Gumroad ($299)**](https://pokhts.gumroad.com/l/senior-engineer-toolkit)

*(Instant Access. One-time payment. Lifetime usage.)*

---

## ğŸ‘¨â€ğŸ’» About the Author
**Phil Yeh** - Senior Automation & Systems Engineer.
Specializing in Hardware-Software Integration, Industrial Automation, and Local AI Solutions.

* [LinkedIn Profile](#)
* [My Gumroad Store](https://pokhts.gumroad.com/)

---
*Keywords: RAG, Llama 3, Ollama, Docker, Local AI, Private GPT, Knowledge Base, Python, Vector Database, ChromaDB, Source Code*
